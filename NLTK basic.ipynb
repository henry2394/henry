{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK là một thư viện Python phổ biến được sử dụng cho NLP.\n",
    "####    -NLP(Natural Language Processing) là khái niệm để chỉ các kĩ thuật, phương pháp thao tác trên ngôn ngữ tự nhiên bằng máy tính.\n",
    "Các ứng dụng cơ bản của NLP :\n",
    "            1. Chế tạo các hệ thống Máy dịch, ví dụ như Google translation.\n",
    "            2. Xử lý văn bản và ngôn ngữ.\n",
    "            3. Chiết suất thông tin.\n",
    "            4. Tóm tắt văn bản.\n",
    "            5. Phân loại văn bản."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So sánh NLTK và 1 số thư viện khác\n",
    "    -Nltk là thư viện chuyên dùng để xử lý và phân tích các văn bản phi cấu trúc, NLTK được sử dụng chủ yếu cho các tác vụ NLP chung (mã hóa, gắn thẻ POS, phân tích cú pháp, v.v.) phục vụ NGHIÊN CỨU HỌC THUẬT do vậy NLTK hỗ trợ nhiều ngôn ngữ khác nhau tuy nhiên đối với dataset lớn thì tốc độ chưa được tối ưu tốt bằng các công cụ khác như Sklearn, Space... và NLTK không có bất kỳ hỗ trợ nào cho việc vectơ từ.\n",
    "    Trong khi đó các công cụ khác như Sklearn, sPacy thì chuyên về HỌC MÁY (classification, clustering, etc.), nó là 1 thư viện có tính hệ thống, cung cấp nhiều mô hình, tính năng mà Nltk chưa có như tf-idf, ngram, classification/clustering method... do được xây dựng chuyên về học máy nên tốc độ có tốt hơn NLTK và dễ sử dụng hơn\n",
    "    \n",
    "    vd: 1 thống kê của thư viện sPacy đã test và đưa ra so sánh về 2 thư viện\n",
    "![image.png](spicy.png)\n",
    "\n",
    "![image.png](compare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import thư viên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic NLP Tasks with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Counting vocabulary of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Hãng tin AFP dẫn lời tổ chức Đài quan sát Nhân quyền Syria (SOHR) đưa tin,\n",
    "ít nhất 17 người đã thiệt mạng trong vụ không kích ngày 25/2 của Mỹ nhằm vào các nhóm dân quân nghi\n",
    "thân Iran trên lãnh thổ Syria. Đây đều là thành viên của các nhóm nói này.\n",
    "Các cuộc tấn công đã phá hủy 3 xe tải chở đạn... Có nhiều thương vong. \n",
    "Thông tin sơ bộ là ít nhất 17 người đã thiệt mạng, giám đốc SOHR Rami Abdul Rahman, cho hay.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import thư viện\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'đã': 3, 'tin': 2, 'nhất': 2, '17': 2, 'người': 2, 'thiệt': 2, 'của': 2, 'các': 2, 'nhóm': 2, 'là': 2, ...})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split đoạn văn bản ra bởi thành 1 list\n",
    "list_text = text.split(' ')\n",
    "count_words = FreqDist(text.split(' '))\n",
    "count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tin', 'nhất', 'người', 'thiệt', 'của', 'các', 'nhóm']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cách để lấy list từ\n",
    "count_words.keys()\n",
    "\n",
    "# cách để lấy số lần xuất hiện \n",
    "count_words['tin']\n",
    "\n",
    "# cách lấy ra các từ xuất hiện nhiều nhât, ví dụ top 3\n",
    "count_words.most_common(3)\n",
    "\n",
    "# cách lựa chọn từ theo nhu cầu, ví dụ độ dài từ >=3, tần suất >1\n",
    "freqwords = [w for w in count_words if len(w) >=3  and count_words[w] > 1]\n",
    "freqwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stemming\n",
    "Stemming là kỹ thuật dùng để biến đổi 1 từ về dạng gốc (được gọi là stem hoặc root form) bằng cách cực kỳ đơn giản là loại bỏ 1 số ký tự nằm ở cuối từ mà nó nghĩ rằng là biến thể của từ. \n",
    "\n",
    "Bởi vì nguyên tắc hoạt động của stemmer rất là đơn giản như vậy cho nên tốc độ xử lý của nó rất là nhanh, và kết quả stem đôi khi không được như chúng ta mong muốn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = \"List listed lists listing listings\"\n",
    "words1 = input1.lower().split(' ')\n",
    "words1\n",
    "words2 = 'go went goes'.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['list', 'list', 'list', 'list', 'list']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "[porter.stem(t) for t in words1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'went', 'goe']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[porter.stem(t) for t in words2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Lemmatization\n",
    "Khác với Stemming là xử lý bằng cách loại bỏ các ký tự cuối từ một cách rất heuristic, Lemmatization sẽ xử lý thông minh hơn bằng một bộ từ điển tuy nhiên tốc độ xử lí k nhanh bằng stemming. Trong các ứng dụng xử lý NLP mà cần độ chính xác cao hơn và thời gian không quan trọng, người ta có thể sử dụng Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'went', 'go']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "[WNlemma.lemmatize(t) for t in words2[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hôm',\n",
       " 'nay',\n",
       " 'trời',\n",
       " 'đẹp',\n",
       " 'ghê',\n",
       " '!',\n",
       " 'chắc',\n",
       " 'nên',\n",
       " 'ra',\n",
       " 'ngoài',\n",
       " '.',\n",
       " 'một',\n",
       " 'chút']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tách văn bản theo while space và stopword\n",
    "nltk.word_tokenize('hôm nay trời đẹp ghê! chắc nên ra ngoài. một chút')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a_little_bit',\n",
       " 'longer',\n",
       " 'than',\n",
       " 'you',\n",
       " 'think,',\n",
       " 'wait',\n",
       " 'me',\n",
       " 'a',\n",
       " 'little,',\n",
       " 'it',\n",
       " 'take',\n",
       " 'time',\n",
       " 'a_lot']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nhận 1 string đã bị tokenize thành từ đơn sau đó maz hóa lại, merge các từ đơn lẻ thành 1 từ bằng cách sử dụng MWE\n",
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')])\n",
    "tokenizer.tokenize('a little bit longer than you think, wait me a little, it take time a lot'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'muffins', 'cost', '$3.88', 'in']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tách văn bản theo regexp\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "s = \"Good muffins cost $3.88 in New York.  Please buy me two of them, Thanks.\"\n",
    "# regexp để nhận kí tự $3.88 và các while space(mặc định)\n",
    "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "tokenizer.tokenize(s)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'muffins', 'cost', '$', '3.88']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(s)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hôm', 'nay.', 'là!', 'thứ', '2@', 'đó']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tách văn bản only by whilespace (mặc định là whilespace và punc)\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "WhitespaceTokenizer().tokenize('hôm nay. là! thứ 2@ đó')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88',\n",
       " 'in New York.  Please buy me',\n",
       " 'two of them.',\n",
       " '',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tách văn bản theo dòng\n",
    "from nltk.tokenize import LineTokenizer\n",
    "s = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "LineTokenizer(blanklines='keep').tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tách từ và chỉnh sửa lại cho đúng từ điển lựa chọn\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "s = \"The colour of the wall is blue.\"\n",
    "# StanfordTokenizer(options={\"americanize\": True}).tokenize(s)\n",
    "\n",
    "### ->> ['The', 'color', 'of', 'the', 'wall', 'is', 'blue', '.']\n",
    "\n",
    "# từ điển stanford có thể dùng cho nhiều ngôn ngữ khác"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hello, i can't feel my feet! Help!!\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detokenzi \n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "toks = ['hello', ',', 'i', 'ca', \"n't\", 'feel', 'my', 'feet', '!', 'Help', '!', '!']\n",
    "twd = TreebankWordDetokenizer()\n",
    "twd.detokenize(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hãng tin AFP dẫn lời tổ chức Đài quan sát Nhân quyền Syria (SOHR) đưa tin,\\nít nhất 17 người đã thiệt mạng trong vụ không kích ngày 25/2 của Mỹ nhằm vào các nhóm dân quân nghi\\nthân Iran trên lãnh thổ Syria.',\n",
       " 'Đây đều là thành viên của các nhóm nói này.',\n",
       " 'Các cuộc tấn công đã phá hủy 3 xe tải chở đạn... Có nhiều thương vong.',\n",
       " 'Thông tin sơ bộ là ít nhất 17 người đã thiệt mạng, giám đốc SOHR Rami Abdul Rahman, cho hay.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tách văn bản theo đoạn văn\n",
    "nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Stopwords\n",
    "- ký tự, từ mà không có ý nghĩa trong câu, nếu không xóa đi thì sẽ ảnh hưởng đến kết quả xử lí (ví dụ: tăng feature lên không cần thiết, khi biến đổi text -> vector làm thay đổi chúng so với các từ cùng loại mà không có stopword...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ex_stopword = \"The government has laid out plans for gradual reopening.\"\n",
    "stop_word = stopwords.words(\"english\")\n",
    "len(stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'government', 'laid', 'plans', 'gradual', 'reopening', '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tkn = word_tokenize(ex_stopword)\n",
    "[x for x in word_tkn if x not in stop_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Part of speech tagging\n",
    "- Xác định danh từ, tính từ, trạng từ ... trong text\n",
    "\n",
    "![image.png](pos.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Children', 'NNP'),\n",
       " ('should', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('drink', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('sugary', 'JJ'),\n",
       " ('drink', 'NN'),\n",
       " ('before', 'IN'),\n",
       " ('bed', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Children shouldn't drink a sugary drink before bed.\"\n",
    "tkn_text = word_tokenize(text)\n",
    "nltk.pos_tag(tkn_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II: Các bước phân loại text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Xử lý features\n",
    "- Loại bỏ stopword (nltk) và punctuation (string library)\n",
    "- Nomalization text bằng cách lowercase hoặc uppercase -> hạn chế sinh ra các feature đồng nghĩa nhưng xuất hiện nhiều lần do cách viết hoa\n",
    "- Stemming hoặc lemmatization đối với tiếng anh. Tiếng việt thì k cần thiết do các từ k có biến thể. Có thể sử dụng underthesea để nối các từ lại vì tiếng việt các từ phải ghép vào mới có nghĩa, ví dụ: Ngôi nhà (2).  Tiếng anh thì chỉ cần House\n",
    "- Nhóm các từ đồng nghĩa ví dụ: (Buy - Purchase) hoặc tiếng việt (bát cơm - tô cơm...)\n",
    "- Convert text sang vector (CountVecterizer, tf-idf, ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Lựa chọn mô hình\n",
    "Các mô hình phân loại phổ biến: Naive Bayes, Supper Vector Machine, Decision tree, LogisticRegression..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1, Bayes \n",
    "Nguyên tắc: Tính xác suất có điều kiện của input. So sánh kết quả -> phân loại input đó vào nhóm có xác suất cao nhất.\n",
    "\n",
    "Công thức cơ bản: probability( y|X) = ( Pr(y) x Pr(X|y) ) / Pr(x)\n",
    "\n",
    "Tuy nhiên Bayes chỉ quan tâm tới cái nào có xác suất cao nhất do đó mô hình này chỉ quan tâm đến argmax Pr(Y) x Pr(X|y). Ví dụ:\n",
    "![image.png](bayes.png)\n",
    "\n",
    "##### Input đầu vào Dear Friends, xác định đây là spam hay không spam, theo công thức ta có:\n",
    "        (1) Pr(Not Spam|Dear Friend) = Pr(Not Spam) * Pr(Dear|Not spam) * Pr(Friend|Not spam) = 0.67 * 0.47 * 0.29 = 0.09\n",
    "        (2) Pr(Spam|Dear Friend) = Pr(Spam) * Pr(Dear|Spam) * Pr(Friend|Spam) = 0.33 * 0.29 * 0.14 = 0.01\n",
    "         => Do xác suất của Not Spam cao hơn => Phân loại nó là Not Spam\n",
    "         \n",
    "#### Bam!!! : Nếu từ cần tìm không có trong tệp dữ liệu, ví dụ: Money thì sẽ xử lý sao ???\n",
    "        Khi đó Pr(Not Spam|dear sir) = Pr(Spam|dear sir) = 0 ??\n",
    "        \n",
    "### ====> Adding dummy count\n",
    "Thêm biến giả sir vào trong tệp dữ liệu, +1 vào các từ đã có sẵn rồi tính toán lại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thời tiết hôm nay rất đẹp</td>\n",
       "      <td>Thời tiết</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tối nay Arsenal đá với ManUnt</td>\n",
       "      <td>Bóng đá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thời tiết hôm nay thật đẹp cho trận đấu này</td>\n",
       "      <td>Bóng đá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trận đấu hôm nay có thể phải diễn ra với thời ...</td>\n",
       "      <td>Bóng đá</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X          Y\n",
       "0                          Thời tiết hôm nay rất đẹp  Thời tiết\n",
       "1                      Tối nay Arsenal đá với ManUnt    Bóng đá\n",
       "2        Thời tiết hôm nay thật đẹp cho trận đấu này    Bóng đá\n",
       "3  Trận đấu hôm nay có thể phải diễn ra với thời ...    Bóng đá"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df['X'] = ['Thời tiết hôm nay rất đẹp', 'Tối nay Arsenal đá với ManUnt', \n",
    "           'Thời tiết hôm nay thật đẹp cho trận đấu này','Trận đấu hôm nay có thể phải diễn ra với thời tiết xấu']\n",
    "df['Y'] = ['Thời tiết', 'Bóng đá','Bóng đá','Bóng đá']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         1],\n",
       "        [1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "         0],\n",
       "        [0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "         1],\n",
       "        [0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "         0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(df.X)\n",
    "X_train = vect.transform(df.X).todense()\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = [1 if i =='Thời tiết' else 0 for i in df.Y]\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB().fit(X_train,Y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "         0]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_predict = 'tối nay thời tiết sẽ mưa'\n",
    "vect.transform([text_predict]).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(vect.transform([text_predict]).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Kiếm tra model\n",
    "- Chia dữ liệu ra làm 2 phần test và phần train.\n",
    "    + from sklearn import model_selection\n",
    "    + x_train, y_train, x_test, y_test = model_selection.train_test_split(training_data,training_label,test_size= 0.75, random_state = 0)\n",
    "- Build model trên list data train\n",
    "    + from sklearn import naive_bayes\n",
    "    + model = naive_bayes.MultinomialNB().fit(train_data, train_label)\n",
    "- Predict dựa trên dữ liệu test rồi so sánh kết quả\n",
    "    + label_predict = model.predict(test_data)\n",
    "    + metrics.f1_score(test_label, label_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
